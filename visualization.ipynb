{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Iy7o1H76RZKI","qMKWc43GSC5u","0StFINCBSy8B"],"authorship_tag":"ABX9TyPtr1rFN/zP5cNEs3mbEfPU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Environment setup"],"metadata":{"id":"Iy7o1H76RZKI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vp2w2a7cRLbx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html"],"metadata":{"id":"euHfxaj4RYnr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/BC/meme/requirements.txt"],"metadata":{"id":"N4zJGIR2RYlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"0x9wlkn4RYi8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Weights & Biases setup"],"metadata":{"id":"qMKWc43GSC5u"}},{"cell_type":"code","source":["import wandb\n","\n","wandb.login(key='YOUR_WANDB_API_KEY')"],"metadata":{"id":"yRsC-gJeRYga"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recreation of reults on HMC dataset\n","\n","For training the model, adjust the '--reproduce t' tag to '--reproduce f'"],"metadata":{"id":"0StFINCBSy8B"}},{"cell_type":"markdown","source":["## HMC dataset"],"metadata":{"id":"5xw4K2icUAdd"}},{"cell_type":"code","source":["!python3 /content/drive/MyDrive/BC/meme/src/main.py --dataset 'hmc' --num_mapping_layers 1 --map_dim 1024 --fusion align --num_pre_output_layers 1 --drop_probs 0.2 0.4 0.1 --gpus '0' --batch_size 64 --lr 0.0001 --max_epochs 50 --name 'text-inv-comb' --pretrained_model 'hmc_text-inv-comb_best.ckpt' --reproduce t --pretrained_proj_weights t --freeze_proj_layers t --comb_proj t --comb_fusion align --convex_tensor f --phi_inv_proj t --text_inv_proj t --post_inv_proj t --enh_text t --phi_freeze t --fast_process t"],"metadata":{"id":"BSmVFOzvRYd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## HARMEME dataset"],"metadata":{"id":"wEX1HwvVUKBp"}},{"cell_type":"code","source":["!python3 /content/drive/MyDrive/BC/meme/src/main.py --dataset 'harmeme' --num_mapping_layers 1 --map_dim 768 --fusion align --num_pre_output_layers 3 --drop_probs 0.2 0.4 0.1 --gpus '0' --batch_size 64 --lr 0.000013 --max_epochs 60 --name 'text-inv-comb' --pretrained_model 'harmeme_text-inv-comb_best.ckpt' --reproduce t --pretrained_proj_weights t --freeze_proj_layers t --comb_proj t --comb_fusion align --convex_tensor f --phi_inv_proj t --text_inv_proj t --post_inv_proj t --enh_text t --phi_freeze t --fast_process t"],"metadata":{"id":"Yk7U6TkbRYbO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualization"],"metadata":{"id":"T89MKjoFT2hD"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/BC/meme/src"],"metadata":{"id":"-XO5TqfSSyZF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualization for random test_unseen HMC meme"],"metadata":{"id":"09ZUmbptVpf0"}},{"cell_type":"code","source":["%matplotlib inline\n","import os\n","import argparse\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from torchvision import transforms\n","\n","from pytorch_lightning import Trainer, seed_everything\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from datasets import MemesCollator, load_dataset\n","from engine import create_model, HateClassifier\n","from utils import str2bool, generate_name\n","\n","from PIL import Image\n","from IPython.display import Image as IPImage, display\n","\n","\n","def get_arg_parser():\n","    parser = argparse.ArgumentParser(description='Training and evaluation script for hateful memes classification')\n","\n","    parser.add_argument('--dataset', default='hmc', choices=['hmc', 'harmeme'])\n","    parser.add_argument('--image_size', type=int, default=224)\n","\n","    parser.add_argument('--num_mapping_layers', default=1, type=int)\n","    parser.add_argument('--map_dim', default=768, type=int)\n","\n","    parser.add_argument('--fusion', default='align',\n","                        choices=['align', 'concat'])\n","\n","    parser.add_argument('--num_pre_output_layers', default=1, type=int)\n","\n","    parser.add_argument('--drop_probs', type=float, nargs=3, default=[0.1, 0.4, 0.2],\n","                        help=\"Set drop probabilities for map, fusion, pre_output\")\n","\n","    parser.add_argument('--gpus', default='0', help='GPU ids concatenated with space')\n","    parser.add_argument('--limit_train_batches', default=1.0)\n","    parser.add_argument('--limit_val_batches', default=1.0)\n","    parser.add_argument('--max_steps', type=int, default=-1)\n","    parser.add_argument('--max_epochs', type=int, default=-1)\n","    parser.add_argument('--log_every_n_steps', type=int, default=25)\n","    parser.add_argument('--val_check_interval', default=1.0)\n","    parser.add_argument('--batch_size', type=int, default=16, help='Batch size')\n","    parser.add_argument('--lr', type=float, default=1e-4)\n","    parser.add_argument('--weight_decay', type=float, default=1e-4)\n","    parser.add_argument('--gradient_clip_val', type=float, default=0.1)\n","\n","    parser.add_argument('--proj_map', default=False, type=str2bool)\n","\n","    parser.add_argument('--pretrained_proj_weights', default=False, type=str2bool)\n","    parser.add_argument('--freeze_proj_layers', default=False, type=str2bool)\n","\n","    parser.add_argument('--comb_proj', default=False, type=str2bool)\n","    parser.add_argument('--comb_fusion', default='align',\n","                        choices=['concat', 'align'])\n","    parser.add_argument('--convex_tensor', default=False, type=str2bool)\n","\n","    parser.add_argument('--text_inv_proj', default=False, type=str2bool)\n","    parser.add_argument('--phi_inv_proj', default=False, type=str2bool)\n","    parser.add_argument('--post_inv_proj', default=False, type=str2bool)\n","\n","    parser.add_argument('--enh_text', default=False, type=str2bool)\n","\n","    parser.add_argument('--phi_freeze', default=False, type=str2bool)\n","\n","    parser.add_argument('--name', type=str, default='adaptation',\n","                        choices=['adaptation', 'hate-clipper', 'image-only', 'text-only', 'sum', 'combiner', 'text-inv',\n","                                 'text-inv-fusion', 'text-inv-comb']\n","                        )\n","    parser.add_argument('--pretrained_model', type=str, default='')\n","    parser.add_argument('--reproduce', default=False, type=str2bool)\n","    parser.add_argument('--print_model', default=False, type=str2bool)\n","    parser.add_argument('--fast_process', default=False, type=str2bool)\n","\n","    return parser\n","\n","\n","# setting the arguments as a string\n","args_str = \"--dataset hmc --image_size 224 --num_mapping_layers 1 --map_dim 1024 --fusion align --num_pre_output_layers 1 --drop_probs 0.2 0.4 0.1 --gpus 0 --limit_train_batches 1.0 --limit_val_batches 1.0 --max_steps -1 --max_epochs -1 --log_every_n_steps 25 --val_check_interval 1.0 --batch_size 16 --lr 0.000013 --weight_decay 1e-4 --gradient_clip_val 0.1 --proj_map False --pretrained_proj_weights t --freeze_proj_layers t --comb_proj t --comb_fusion align --convex_tensor False --text_inv_proj t --phi_inv_proj t --post_inv_proj t --enh_text t --phi_freeze t --name adaptation --pretrained_model 'hmc_text-inv-comb_best.ckpt' --reproduce t --print_model False --fast_process t\"\n","\n","# parsing the arguments\n","arguments = get_arg_parser().parse_args(args_str.split())\n","\n","# setting the GPU ids\n","arguments.gpus = [int(id_) for id_ in arguments.gpus.split()]\n","\n","# loading dataset and creating dataloader\n","dataset_test = load_dataset(args=arguments, split='test_unseen')\n","collator = MemesCollator(arguments)\n","dataloader_test = DataLoader(dataset_test, batch_size=arguments.batch_size, collate_fn=collator, num_workers=0)\n","print(\"Number of samples in the dataset:\", len(dataloader_test.dataset))\n","\n","# loading or reproducing of trained model\n","model = create_model(arguments)"],"metadata":{"id":"6YCRMjXEdiPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MemesDataset(Dataset):\n","    def __init__(self, root_folder, dataset, split='train', image_size=224):\n","        super(MemesDataset, self).__init__()\n","        self.root_folder = root_folder\n","        self.dataset = dataset\n","        self.split = split\n","        self.image_size = image_size\n","\n","        self.info_file = os.path.join(root_folder, dataset, f'/content/drive/MyDrive/BC/meme/resources/datasets/hmc/labels/hmc_info.csv')\n","        self.df = pd.read_csv(self.info_file)\n","        self.df = self.df[self.df['split'] == self.split].reset_index(drop=True)\n","        float_cols = self.df.select_dtypes(float).columns\n","        self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        if row['text'] == 'nothing':\n","            txt = 'null'\n","        else:\n","            txt = row['text']\n","\n","        if self.dataset == 'hmc':\n","            image_fn = row['img'].split('/')[1]\n","        else:\n","            image_fn = row['img']\n","\n","        image_path = os.path.join(self.root_folder, image_fn)\n","        image = Image.open(image_path).convert('RGB').resize((self.image_size, self.image_size))\n","\n","        item = {\n","            'image': image,\n","            'text': txt,\n","            'label': row['label'],\n","            'idx_meme': row['id'],\n","            'origin_text': txt\n","        }\n","\n","        return item\n","\n","\n","# loading of the dataset\n","dataset_test = MemesDataset(root_folder='/content/drive/MyDrive/BC/meme/resources/datasets/hmc', dataset='img', split='test_unseen')"],"metadata":{"id":"eDpvEtemd0Yb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_random_image_with_prediction(dataset, model):\n","    # setting the model to evaluation mode\n","    model.eval()\n","\n","    # selecting a random index\n","    idx = random.randint(0, len(dataset) - 1)\n","\n","    # retrieving the sample from the dataset\n","    sample = dataset[idx]\n","\n","    # getting the image, label, and text from the sample\n","    image = sample['image']\n","    label = sample['label']\n","    text = sample['text']\n","\n","    # preparation of the image for model prediction\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    # adding batch dimension\n","    input_image = transform(image).unsqueeze(0)\n","\n","    # making predictions using the model\n","    with torch.no_grad():\n","        model_output = model({'images': input_image, 'texts': text, 'labels': label})\n","\n","    # extracting the predicted label from the model output\n","    predicted_label = int(torch.round(torch.sigmoid(model_output['logits'])).item())\n","\n","    # converting the PIL image to a NumPy array for display\n","    image_np = np.array(image)\n","    image_np = image_np / 255.0\n","    image_np = np.clip(image_np, 0, 1)\n","\n","    # displaying the image with the true label and predicted label\n","    plt.imshow(image_np)\n","    plt.title(f\"True Label: {label}, Predicted Label: {predicted_label}\")\n","    plt.show()\n","\n","\n","# usage\n","show_random_image_with_prediction(dataset_test, model)"],"metadata":{"id":"2wvS1DOQd4i9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualization for random test_unseen HARMEME meme"],"metadata":{"id":"Fpx-YkAgdD4i"}},{"cell_type":"code","source":["%matplotlib inline\n","import os\n","import argparse\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from torchvision import transforms\n","\n","from pytorch_lightning import Trainer, seed_everything\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from datasets import MemesCollator, load_dataset\n","from engine import create_model, HateClassifier\n","from utils import str2bool, generate_name\n","\n","from PIL import Image\n","from IPython.display import Image as IPImage, display\n","\n","\n","def get_arg_parser():\n","    parser = argparse.ArgumentParser(description='Training and evaluation script for hateful memes classification')\n","\n","    parser.add_argument('--dataset', default='hmc', choices=['hmc', 'harmeme'])\n","    parser.add_argument('--image_size', type=int, default=224)\n","\n","    parser.add_argument('--num_mapping_layers', default=1, type=int)\n","    parser.add_argument('--map_dim', default=768, type=int)\n","\n","    parser.add_argument('--fusion', default='align',\n","                        choices=['align', 'concat'])\n","\n","    parser.add_argument('--num_pre_output_layers', default=1, type=int)\n","\n","    parser.add_argument('--drop_probs', type=float, nargs=3, default=[0.1, 0.4, 0.2],\n","                        help=\"Set drop probabilities for map, fusion, pre_output\")\n","\n","    parser.add_argument('--gpus', default='0', help='GPU ids concatenated with space')\n","    parser.add_argument('--limit_train_batches', default=1.0)\n","    parser.add_argument('--limit_val_batches', default=1.0)\n","    parser.add_argument('--max_steps', type=int, default=-1)\n","    parser.add_argument('--max_epochs', type=int, default=-1)\n","    parser.add_argument('--log_every_n_steps', type=int, default=25)\n","    parser.add_argument('--val_check_interval', default=1.0)\n","    parser.add_argument('--batch_size', type=int, default=16, help='Batch size')\n","    parser.add_argument('--lr', type=float, default=1e-4)\n","    parser.add_argument('--weight_decay', type=float, default=1e-4)\n","    parser.add_argument('--gradient_clip_val', type=float, default=0.1)\n","\n","    parser.add_argument('--proj_map', default=False, type=str2bool)\n","\n","    parser.add_argument('--pretrained_proj_weights', default=False, type=str2bool)\n","    parser.add_argument('--freeze_proj_layers', default=False, type=str2bool)\n","\n","    parser.add_argument('--comb_proj', default=False, type=str2bool)\n","    parser.add_argument('--comb_fusion', default='align',\n","                        choices=['concat', 'align'])\n","    parser.add_argument('--convex_tensor', default=False, type=str2bool)\n","\n","    parser.add_argument('--text_inv_proj', default=False, type=str2bool)\n","    parser.add_argument('--phi_inv_proj', default=False, type=str2bool)\n","    parser.add_argument('--post_inv_proj', default=False, type=str2bool)\n","\n","    parser.add_argument('--enh_text', default=False, type=str2bool)\n","\n","    parser.add_argument('--phi_freeze', default=False, type=str2bool)\n","\n","    parser.add_argument('--name', type=str, default='adaptation',\n","                        choices=['adaptation', 'hate-clipper', 'image-only', 'text-only', 'sum', 'combiner', 'text-inv',\n","                                 'text-inv-fusion', 'text-inv-comb']\n","                        )\n","    parser.add_argument('--pretrained_model', type=str, default='')\n","    parser.add_argument('--reproduce', default=False, type=str2bool)\n","    parser.add_argument('--print_model', default=False, type=str2bool)\n","    parser.add_argument('--fast_process', default=False, type=str2bool)\n","\n","    return parser\n","\n","\n","# setting the arguments as a string\n","args_str = \"--dataset harmeme --image_size 224 --num_mapping_layers 1 --map_dim 768 --fusion align --num_pre_output_layers 3 --drop_probs 0.2 0.4 0.1 --gpus 0 --limit_train_batches 1.0 --limit_val_batches 1.0 --max_steps -1 --max_epochs -1 --log_every_n_steps 25 --val_check_interval 1.0 --batch_size 16 --lr 0.000013 --weight_decay 1e-4 --gradient_clip_val 0.1 --proj_map False --pretrained_proj_weights t --freeze_proj_layers t --comb_proj t --comb_fusion align --convex_tensor False --text_inv_proj t --phi_inv_proj t --post_inv_proj t --enh_text t --phi_freeze t --name adaptation --pretrained_model 'harmeme_text-inv-comb_best.ckpt' --reproduce t --print_model False --fast_process t\"\n","\n","# parsing the arguments\n","arguments = get_arg_parser().parse_args(args_str.split())\n","\n","# setting the GPU ids\n","arguments.gpus = [int(id_) for id_ in arguments.gpus.split()]\n","\n","# loading dataset and creating dataloader\n","dataset_test = load_dataset(args=arguments, split='test')\n","collator = MemesCollator(arguments)\n","dataloader_test = DataLoader(dataset_test, batch_size=arguments.batch_size, collate_fn=collator, num_workers=0)\n","print(\"Number of samples in the dataset:\", len(dataloader_test.dataset))\n","\n","# loading or reproducing of trained model\n","model = create_model(arguments)"],"metadata":{"id":"NGRVCLRnRYOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MemesDataset(Dataset):\n","    def __init__(self, root_folder, dataset, split='train', image_size=224):\n","        super(MemesDataset, self).__init__()\n","        self.root_folder = root_folder\n","        self.dataset = dataset\n","        self.split = split\n","        self.image_size = image_size\n","\n","        self.info_file = os.path.join(root_folder, dataset, f'/content/drive/MyDrive/BC/meme/resources/datasets/harmeme/labels/harmeme_info.csv')\n","        self.df = pd.read_csv(self.info_file)\n","        self.df = self.df[self.df['split'] == self.split].reset_index(drop=True)\n","        float_cols = self.df.select_dtypes(float).columns\n","        self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        if row['text'] == 'nothing':\n","            txt = 'null'\n","        else:\n","            txt = row['text']\n","\n","        if self.dataset == 'hmc':\n","            image_fn = row['img'].split('/')[1]\n","        else:\n","            image_fn = row['image']\n","\n","        image_path = os.path.join(self.root_folder, image_fn)\n","        image = Image.open(image_path).convert('RGB').resize((self.image_size, self.image_size))\n","\n","        item = {\n","            'image': image,\n","            'text': txt,\n","            'label': row['label'],\n","            'idx_meme': row['id'],\n","            'origin_text': txt\n","        }\n","\n","        return item\n","\n","\n","# loading of the dataset\n","dataset_test = MemesDataset(root_folder='/content/drive/MyDrive/BC/meme/resources/datasets/harmeme/img', dataset='img', split='test')"],"metadata":{"id":"3UGnFSagdDU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_random_image_with_prediction(dataset, model):\n","    # setting the model to evaluation mode\n","    model.eval()\n","\n","    # selecting a random index\n","    idx = random.randint(0, len(dataset) - 1)\n","\n","    # retrieving the sample from the dataset\n","    sample = dataset[idx]\n","\n","    # getting the image, label, and text from the sample\n","    image = sample['image']\n","    label = sample['label']\n","    text = sample['text']\n","\n","    # preparation of the image for model prediction\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    # adding batch dimension\n","    input_image = transform(image).unsqueeze(0)\n","\n","    # making predictions using the model\n","    with torch.no_grad():\n","        model_output = model({'images': input_image, 'texts': text, 'labels': label})\n","\n","    # extracting the predicted label from the model output\n","    predicted_label = int(torch.round(torch.sigmoid(model_output['logits'])).item())\n","\n","\n","    # converting the PIL image to a NumPy array for display\n","    image_np = np.array(image)\n","    image_np = image_np / 255.0  # Normalize to [0, 1]\n","    image_np = np.clip(image_np, 0, 1)  # Ensure the image is in the valid range for display\n","\n","    # displaying the image with the ground truth and predicted labels\n","    plt.imshow(image_np)\n","    plt.title(f\"True Label: {label}, Predicted Label: {predicted_label}\")\n","    plt.show()\n","\n","\n","# usage\n","show_random_image_with_prediction(dataset_test, model)"],"metadata":{"id":"VDoXPmztdDjj"},"execution_count":null,"outputs":[]}]}